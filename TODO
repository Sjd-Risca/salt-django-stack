TODO

- sync db with postgresql and set django settings to use them

- some psql info here:
/home/alexis/Documents/zinibu-data/zinibu-articles/development/linkin-vim-files:This is in this file [how-to-pass-password-psql]
/home/alexis/Documents/zinibu-data/zinibu-articles/development/django-test-db-psql-permission:(django13)alexis@amber:~/djcode/marketr$ sudo -u postgres psql
/home/alexis/Documents/zinibu-data/zinibu-articles/development/django-test-db-psql-permission:psql (8.4.8)
/home/alexis/Documents/zinibu-data/zinibu-articles/development/how-to-pass-password-psql:psql -h localhost -d playwithdjango
/home/alexis/Documents/zinibu-data/zinibu-articles/development/psql-database-setup:$ sudo -u postgres psql
/home/alexis/Documents/zinibu-data/zinibu-articles/development/psql-database-setup:psql -U alexis -d cataybea -f cataybea.sql
/home/alexis/Documents/zinibu-data/zinibu-articles/development/psql-database-setup:psql -d cataybea
/home/alexis/Documents/zinibu-data/zinibu-articles/development/dump-pg-database:$ psql -d payroll -f payroll.dump.out
/home/alexis/Documents/zinibu-data/zinibu-articles/development/fabric-deployment:dump/restore pg db [how-to-pass-password-psql]
/home/alexis/Documents/zinibu-data/zinibu-articles/development/fabric-deployment:psql -h 192.168.0.182 -d alexistestdb -U alexis
/home/alexis/Documents/zinibu-data/zinibu-articles/development/fabric-deployment:more on [psql-stuff]
/home/alexis/Documents/zinibu-data/zinibu-articles/to-polish/data/psql-stuff:more about psql on [fabric-deployment]


- need to run a state that accepts a file path as parameter to load db dump from tar.gz file in one directory into postgresql server, start test with any file that's copied an uncompressed, see my example of passing pillar via command line

- find a way to pass the environment to use for starting Django, which will determine the value of DJANGO_SETTINGS_MODULE used when --settings not passed to django-admin.py and used automatically when calling gunicorn, for now,using $PROJECTNAME.settings.local
- postgresql user and db are already being created, need to update django settings correctly, use basic settings for now, later I'll move to scoops-style settings
- at this point, all settings in django should be set by salt
- detect Ubuntu 14.10 to setup vcl for Varnish 4.0, test it manually first: https://www.varnish-cache.org/docs/4.0/whats-new/upgrading.html

- try again with Digital Ocean and 1 Gb. RAM for Varnish
- try keepalived with digital ocean tutorial, Python script is saved locally

- remove unused data from pillar zinibu_haproxy and zinibu_postgresql and any others

- see about using state.orchestrate for deployment, to make sure code is deployed for all minions before services are restarted
- States for deployment, probably named zinibu.deploy, and find best way to indicate if it's development, staging, production (pass via pillar in command line, see README and python states). I think I just need to consider one box as one environment so minions' ids are enough to target the states. This means no more having dev, staging and production all on the same box. Run from command line after merged to a branch I designate as the one feeding the environment, like "production" and "staging," and probably use git hooks so after something is merged to any of those branches the autodeployment runs

- move postgres-related lines from zinibu.python.init to postgres specific states, add include to zinibu.python.init.

- don't worry about high availability for redis and postgresql yet
- keepalived won't be used at the beginning for ha haproxy
- most destructive operations (umount, removing directories, uninstalls, etc) should be handled manually to avoid errors
- install redis and change settings to use it
- modifying settings.py in django to connect to db and dbsync/migrate as needed, see django formula for ideas

- logrotate to keep all logs under control (syslog-ng for something else?)

- I want to run the whole thing with:
sudo salt '*' state.highstate


- upgrade to varnish 4?

- I may continue without the keepalived shit. If I shutdown the keepalived service, it works. The problem is that backup is not becoming master as it shoud. The check script is working and priority is changing but still master remains master.

- Check about multicast, unicast, firewall and communicating between hosts with keepalived

http://serverfault.com/questions/512153/both-servers-running-keepalived-become-master-and-have-a-same-virtual-ip
http://www.cyberciti.biz/faq/linux-unix-verify-keepalived-working-or-not/

ping vrrp.mcast.net
iptables -L
sudo iptables -L
sudo tcpdump -vvv -n -i eth0 host 224.0.0.18
sudo iptables -I INPUT -i eth0 -d 224.0.0.0/8 -j ACCEPT
sudo iptables -L
sudo iptables -A INPUT -p 112 -i eth0 -j ACCEPT
sudo iptables -L
sudo iptables -A OUTPUT -p 112 -o eth0 -j ACCEPT
sudo iptables -L
sudo tcpdump -vvv -n -i eth0 host 224.0.0.18
sudo tcpdump -v -i eth0 host 224.0.0.18
sudo service keepalived restart
sudo service haproxy status


- check connections
netstat -ctnup | grep "192.168.1.95"

===
Linode tests

10/26/15 After Linode test 2:
- fix settings to use correct user, db info and more from pillar, see zinibu_dev/settings.py:STATIC_ROOT = '/home/vagrant/zinibu_dev/static'

10/25/15 After Linode test 1 ($ 0.33):
- focus on 14.04 LTS, 15.04 has replaced upstart with systemd and I don't want to mess with that for now, eventually I'll update these salt formulas to make a Django project run with systemd
====

